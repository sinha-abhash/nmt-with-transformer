{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28bd5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22f2c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 15:22:35.333495: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 15:22:36.179763: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.8/lib64:\n",
      "2023-02-07 15:22:36.179839: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:/usr/local/cuda-11.8/lib64:\n",
      "2023-02-07 15:22:36.179846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892fc666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 15:22:36.983814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 15:22:37.003586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 15:22:37.003824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 15:22:37.004815: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 15:22:37.005772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 15:22:37.005957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 15:22:37.006110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 15:22:37.434266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 15:22:37.434463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 15:22:37.434616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-07 15:22:37.434750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6315 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:2d:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n",
    "                               with_info=True,\n",
    "                               as_supervised=True)\n",
    "\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8de8381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Examples in Portuguese:\n",
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "\n",
      "mas e se estes fatores fossem ativos ?\n",
      "\n",
      "mas eles não tinham a curiosidade de me testar .\n",
      "\n",
      "> Examples in English:\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 15:22:37.691239: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "    print('> Examples in Portuguese:')\n",
    "    for pt in pt_examples.numpy():\n",
    "        print(pt.decode('utf-8'))\n",
    "        print()\n",
    "\n",
    "    print('> Examples in English:')\n",
    "    for en in en_examples.numpy():\n",
    "        print(en.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449a1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ted_hrlr_translate_pt_en_converter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.get_file(\n",
    "    f'{model_name}.zip',\n",
    "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "025be899",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.saved_model.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f5039f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detokenize',\n",
       " 'get_reserved_tokens',\n",
       " 'get_vocab_path',\n",
       " 'get_vocab_size',\n",
       " 'lookup',\n",
       " 'tokenize',\n",
       " 'tokenizer',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defae7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> This is a batch of strings:')\n",
    "for en in en_examples.numpy():\n",
    "    print(en.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4408389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> This is a padded-batch of token IDs:\n",
      "[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n",
      "[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n",
      "[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizers.en.tokenize(en_examples)\n",
    "\n",
    "print('> This is a padded-batch of token IDs:')\n",
    "for row in encoded.to_list():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c6374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> This is human-readable text:\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n ' t test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "round_trip = tokenizers.en.detokenize(encoded)\n",
    "\n",
    "print('> This is human-readable text:')\n",
    "for line in round_trip.numpy():\n",
    "    print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c824541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> This is the text split into tokens:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',\n",
       "  b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',\n",
       "  b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',\n",
       "  b'##ity', b'.', b'[END]']                                                 ,\n",
       " [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',\n",
       "  b'[END]']                                                           ,\n",
       " [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for',\n",
       "  b'curiosity', b'.', b'[END]']                                          ]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('> This is the text split into tokens:')\n",
    "tokens = tokenizers.en.lookup(encoded)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f071df9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................."
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "for pt_examples, en_examples in train_examples.batch(1024):\n",
    "    pt_tokens = tokenizers.pt.tokenize(pt_examples)\n",
    "    lengths.append(pt_tokens.row_lengths())\n",
    "\n",
    "    en_tokens = tokenizers.en.tokenize(en_examples)\n",
    "    lengths.append(en_tokens.row_lengths())\n",
    "    print('.', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccd60d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_lengths = np.concatenate(lengths)\n",
    "\n",
    "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
    "plt.ylim(plt.ylim())\n",
    "max_length = max(all_lengths)\n",
    "plt.plot([max_length, max_length], plt.ylim())\n",
    "plt.title(f'Maximum tokens per example: {max_length}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db02ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS=128\n",
    "def prepare_batch(pt, en):\n",
    "    pt = tokenizers.pt.tokenize(pt)      # Output is ragged.\n",
    "    pt = pt[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
    "    pt = pt.to_tensor()  # Convert to 0-padded dense Tensor\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    en = en[:, :(MAX_TOKENS+1)]\n",
    "    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens\n",
    "\n",
    "    return (pt, en_inputs), en_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "454a0331",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e753c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "    return (\n",
    "      ds\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7afa43b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation set batches.\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b78be20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 95)\n",
      "(64, 93)\n",
      "(64, 93)\n"
     ]
    }
   ],
   "source": [
    "for (pt, en), en_labels in train_batches.take(1):\n",
    "    break\n",
    "\n",
    "print(pt.shape)\n",
    "print(en.shape)\n",
    "print(en_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbcdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "\n",
    "# Check the shape.\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d35770",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\n",
    "p = pos_encoding[1000]\n",
    "dots = tf.einsum('pd,d -> p', pos_encoding, p)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(dots)\n",
    "plt.ylim([0,1])\n",
    "plt.plot([950, 950, float('nan'), 1050, 1050],\n",
    "         [0,1,float('nan'),0,1], color='k', label='Zoom')\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(dots)\n",
    "plt.xlim([950, 1050])\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c8ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bf69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_pt = PositionalEmbedding(vocab_size=tokenizers.pt.get_vocab_size(), d_model=512)\n",
    "embed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)\n",
    "\n",
    "pt_emb = embed_pt(pt)\n",
    "en_emb = embed_en(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b98a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_emb._keras_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdffd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(pt_emb.shape)\n",
    "print(en_emb.shape)\n",
    "print(sample_ca(en_emb, pt_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3143fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(pt_emb.shape)\n",
    "print(sample_gsa(pt_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11888e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c354a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(en_emb.shape)\n",
    "print(sample_csa(en_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = sample_csa(embed_en(en[:, :3])) \n",
    "out2 = sample_csa(embed_en(en))[:, :3]\n",
    "\n",
    "tf.reduce_max(abs(out1 - out2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf57478",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ffn = FeedForward(512, 2048)\n",
    "\n",
    "print(en_emb.shape)\n",
    "print(sample_ffn(en_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5624c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "print(pt_emb.shape)\n",
    "print(sample_encoder_layer(pt_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1edf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the encoder.\n",
    "sample_encoder = Encoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=8500)\n",
    "\n",
    "sample_encoder_output = sample_encoder(pt, training=False)\n",
    "\n",
    "# Print the shape.\n",
    "print(pt.shape)\n",
    "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "sample_decoder_layer_output = sample_decoder_layer(\n",
    "    x=en_emb, context=pt_emb)\n",
    "\n",
    "print(en_emb.shape)\n",
    "print(pt_emb.shape)\n",
    "print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                         dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "          x  = self.dec_layers[i](x, context)\n",
    "\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120accb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the decoder.\n",
    "sample_decoder = Decoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=8000)\n",
    "\n",
    "output = sample_decoder(\n",
    "    x=en,\n",
    "    context=pt_emb)\n",
    "\n",
    "# Print the shapes.\n",
    "print(en.shape)\n",
    "print(pt_emb.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eaff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "        # first argument.\n",
    "        context, x  = inputs\n",
    "\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "        x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "        # Final linear layer output.\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Return the final output and the attention weights.\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d3b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aeebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer((pt, en))\n",
    "\n",
    "print(en.shape)\n",
    "print(pt.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35415ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.shape, en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed042101",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
    "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405bbcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a08d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(train_batches,\n",
    "                epochs=20,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05fe053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(self, tokenizers, transformer):\n",
    "        self.tokenizers = tokenizers\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "        # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "        assert isinstance(sentence, tf.Tensor)\n",
    "        if len(sentence.shape) == 0:\n",
    "            sentence = sentence[tf.newaxis]\n",
    "\n",
    "        sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "        encoder_input = sentence\n",
    "\n",
    "        # As the output language is English, initialize the output with the\n",
    "        # English `[START]` token.\n",
    "        start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "        start = start_end[0][tf.newaxis]\n",
    "        end = start_end[1][tf.newaxis]\n",
    "\n",
    "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "            # Select the last token from the `seq_len` dimension.\n",
    "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "            # Concatenate the `predicted_id` to the output which is given to the\n",
    "            # decoder as its input.\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "            if predicted_id == end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # The output shape is `(1, tokens)`.\n",
    "        text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
    "\n",
    "        tokens = tokenizers.en.lookup(output)[0]\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop.\n",
    "        # So, recalculate them outside the loop.\n",
    "        self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, tokens, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02296c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'este é um problema que temos que resolver.'\n",
    "ground_truth = 'this is a problem we have to solve .'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'os meus vizinhos ouviram sobre esta ideia.'\n",
    "ground_truth = 'and my neighboring homes heard about this idea .'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'este é o primeiro livro que eu fiz.'\n",
    "ground_truth = \"this is the first book i've ever done.\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679bc5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
    "  # The model didn't generate `<START>` in the output. Skip it.\n",
    "    translated_tokens = translated_tokens[1:]\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.matshow(attention)\n",
    "    ax.set_xticks(range(len(in_tokens)))\n",
    "    ax.set_yticks(range(len(translated_tokens)))\n",
    "\n",
    "    labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
    "    ax.set_xticklabels(\n",
    "      labels, rotation=90)\n",
    "\n",
    "    labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
    "    ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f66531",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = 0\n",
    "# Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.\n",
    "attention_heads = tf.squeeze(attention_weights, 0)\n",
    "attention = attention_heads[head]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tokens = tf.convert_to_tensor([sentence])\n",
    "in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
    "in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
    "in_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ccff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_head(in_tokens, translated_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac969ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
    "    in_tokens = tf.convert_to_tensor([sentence])\n",
    "    in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
    "    in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    for h, head in enumerate(attention_heads):\n",
    "        ax = fig.add_subplot(2, 4, h+1)\n",
    "\n",
    "        plot_attention_head(in_tokens, translated_tokens, head)\n",
    "\n",
    "        ax.set_xlabel(f'Head {h+1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_weights(sentence,\n",
    "                       translated_tokens,\n",
    "                       attention_weights[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37daac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Eu li sobre triceratops na enciclopédia.'\n",
    "ground_truth = 'I read about triceratops in the encyclopedia.'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)\n",
    "\n",
    "plot_attention_weights(sentence, translated_tokens, attention_weights[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef11362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportTranslator(tf.Module):\n",
    "    def __init__(self, translator):\n",
    "        self.translator = translator\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "    def __call__(self, sentence):\n",
    "        (result,\n",
    "         tokens,\n",
    "         attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93293622",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = ExportTranslator(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator('este é o primeiro livro que eu fiz.').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977db74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(translator, export_dir='translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c75fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.saved_model.load('translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded('este é o primeiro livro que eu fiz.').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca0447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
